
## Data


#### 1. Download the MIMIC-Diff-VQA dataset from Physionet(under review) and place all files into the ./data directory.
```
./data/mimic_all.csv
./data/all_diseases.json
./data/mimic_pair_questions.csv
```

#### 2. Ensure the following files exist. The top two files are generated by the MIMIC-Diff-VQA generation code. The last two files contain the corresponding relationships between dicom id and the study id in the MIMIC-CXR dataset.
```
./data/vocab_mimic_VQA.json
./data/splits_mimic_VQA.json
./data/dicom2id.pkl
./data/study2dicom.pkl
```
#### 3. Execute:
```angular2html
python dataset_preparation.py -t -c
```
This will produce the files listed below. The top three files are the ground truth answers in the coco format used for evaluation. The last file is the hdf5 file that contains the questions and answers in a format that the model can use directly.
```
./data/mimic_gt_captions_test.json
./data/mimic_gt_captions_val.json
./data/mimic_gt_captions_train.json
./data/VQA_mimic_dataset.h5
```



#### 4. Execute the feature extraction code. The feature file below will automatically be placed into the ./data folder. Please refer to the README.md in the [../feature extraction](../feature extraction) folder for more details.
```
./data/cmb_bbox_di_feats.hdf5
```

## Training
```
python train_mimic.py --graph all --use_wandb False --eval_target val
```
--graph: all, implicit, spatial, semantic

--use_wandb: True, False. (wandb is a tool for tracking training process. If you don't want to use it, set it to False.)

--eval_target: val, test. (choose the set for evaluation during the training. val is for validation set, test is for test set)


## Testing
Testing has been integerated in the training code, but it can also be performed separately.

1. Download the provided checkpoint file using this [link](https://drive.google.com/file/d/1ZDOnAD0qOx82UPqRISprH5Y3w5qLnYV-/view?usp=drive_link), or load your own checkpoint file.

2. run the testing code and specify the path of the checkpoint file.
```angular2html
python test_mimic.py -p <path_to_checkpoint_file> 
```





## (optional) Retrieve the history score 
Evaluation score for each checkpoint can be retrieved by executing the following code.
```angular2html
python evaluate_score.py -n <run_name> -c <checkpoint_num>
```
run_name: the name of the training run. (can be found at the beginning of the training.)

checkpoint_num: the iteration number of the checkpoint. (This should be multiple of the interval. the default interval is 2000)





